# robots.txt for ben-del-cmd.github.io/p-feeding
# 目的：允许正常索引；屏蔽内部测试页；明确指向站点地图。

User-agent: *
Allow: /

# —— 不希望收录的页面/目录（按需增减）——
Disallow: /test.html
# Disallow: /draft/
# Disallow: /private/
# Disallow: /weight.html   # 示例：未上线前先屏蔽

# —— 站点地图 —— 
Sitemap: https://ben-del-cmd.github.io/p-feeding/sitemap.xml

# —— 进阶可选：按机器人定制（默认注释，按需启用）——
# User-agent: Googlebot-Image
# Allow: /

# User-agent: AdsBot-Google
# Disallow: /

# User-agent: AhrefsBot
# Crawl-delay: 5

# 备注：
# 1) GitHub Pages 会直接按此文件生效；提交到 main 后几分钟内即可访问。
# 2) 每次新增/下线页面，请同步更新 sitemap.xml 与此文件（如需屏蔽）。
